{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "693808dc-97a6-4d9c-bb4b-c5d6fbc71c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser\n",
    "from copy import deepcopy\n",
    "from typing import Any, Union\n",
    "\n",
    "import torch\n",
    "from pytorch_lightning import LightningModule, Trainer, seed_everything\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import Adam\n",
    "\n",
    "from transformers import BertModel, BertConfig\n",
    "\n",
    "from pl_bolts.callbacks.byol_updates import BYOLMAWeightUpdate\n",
    "from pl_bolts.models.self_supervised.byol.models import SiameseArm\n",
    "from pl_bolts.optimizers.lr_scheduler import LinearWarmupCosineAnnealingLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "477a449f-9926-48ca-8402-3cd2d97c6539",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data2Vec(LightningModule):\n",
    "    \"\"\"PyTorch Lightning implementation of Data2Vec by Meta AI.\n",
    "    \n",
    "    Codebase extended from BYOL implementaion of `Annika Brundyn <https://github.com/annikabrundyn>` \\\n",
    "    found at https://github.com/PyTorchLightning/lightning-bolts/tree/master/pl_bolts/models/self_supervised/byol\n",
    "\n",
    "    Model implemented by:\n",
    "        - `Haris Jabbar <https://github.com/maveriq>`_\n",
    "\n",
    "    .. warning:: Work in progress. This implementation is still being verified.\n",
    "\n",
    "    TODOs:\n",
    "        - Implement data augmentation pipeline\n",
    "        - Verify Implementation\n",
    "        - Implement selectable top K layers instead of all (current)\n",
    "\n",
    "    Example::\n",
    "\n",
    "        model = Data2Vec()\n",
    "\n",
    "        dm = ///tobeimplemented...\n",
    "\n",
    "        trainer = pl.Trainer()\n",
    "        trainer.fit(model, datamodule=dm)\n",
    "\n",
    "    Train::\n",
    "\n",
    "        trainer = Trainer()\n",
    "        trainer.fit(model)\n",
    "\n",
    "    CLI command::\n",
    "\n",
    "\n",
    "    .. _BYOL: https://arxiv.org/pdf/2006.07733.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes,\n",
    "        learning_rate: float = 0.2,\n",
    "        weight_decay: float = 1.5e-6,\n",
    "        input_height: int = 32,\n",
    "        batch_size: int = 32,\n",
    "        num_workers: int = 0,\n",
    "        warmup_epochs: int = 10,\n",
    "        max_epochs: int = 1000,\n",
    "        # base_encoder: Union[str, torch.nn.Module] = \"resnet50\",\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            datamodule: The datamodule\n",
    "            learning_rate: the learning rate\n",
    "            weight_decay: optimizer weight decay\n",
    "            input_height: image input height\n",
    "            batch_size: the batch size\n",
    "            num_workers: number of workers\n",
    "            warmup_epochs: num of epochs for scheduler warm up\n",
    "            max_epochs: max epochs for scheduler\n",
    "            base_encoder: the base encoder module or resnet name\n",
    "            encoder_out_dim: output dimension of base_encoder\n",
    "            projector_hidden_size: hidden layer size of projector MLP\n",
    "            projector_out_dim: output size of projector MLP\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(ignore=\"base_encoder\")\n",
    "        \n",
    "        config = BertConfig.from_pretrained('bert-base-uncased')\n",
    "        config.output_hidden_states=True\n",
    "\n",
    "        self.teacher_network = BertModel(config,)\n",
    "        self.student_network = deepcopy(self.teacher_network)\n",
    "        self.weight_callback = BYOLMAWeightUpdate()\n",
    "        self.loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "    def on_train_batch_end(self, outputs, batch: Any, batch_idx: int, dataloader_idx: int) -> None:\n",
    "        # Add callback for user automatically since it's key to BYOL weight update\n",
    "        self.weight_callback.on_train_batch_end(self.trainer, self, outputs, batch, batch_idx, dataloader_idx)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.online_network(x)\n",
    "        return y\n",
    "\n",
    "    def shared_step(self, batch, batch_idx):\n",
    "        raw_input, masked_input = batch\n",
    "\n",
    "        # Image 1 to image 2 loss\n",
    "        output_student = self.student_network(img_1)\n",
    "        with torch.no_grad():\n",
    "            output_teacher = self.teacher_network(img_2)\n",
    "\n",
    "        embed_student = torch.cat(output_student.hidden_states,0).mean(0)\n",
    "        embed_teacher = torch.cat(output_teacher.hidden_states,0).mean(0)\n",
    "        # Final loss\n",
    "        loss = self.loss_fn(embed_student, embed_teacher)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self.shared_step(batch, batch_idx)\n",
    "\n",
    "        # log results\n",
    "        self.log({\"train_loss\": loss})\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self.shared_step(batch, batch_idx)\n",
    "\n",
    "        # log results\n",
    "        self.log({\"valid_loss\": loss})\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = Adam(self.parameters(), lr=self.hparams.learning_rate, weight_decay=self.hparams.weight_decay)\n",
    "        scheduler = LinearWarmupCosineAnnealingLR(\n",
    "            optimizer, warmup_epochs=self.hparams.warmup_epochs, max_epochs=self.hparams.max_epochs\n",
    "        )\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    @staticmethod\n",
    "    def add_model_specific_args(parent_parser):\n",
    "        parser = ArgumentParser(parents=[parent_parser], add_help=False)\n",
    "        parser.add_argument(\"--online_ft\", action=\"store_true\", help=\"run online finetuner\")\n",
    "        parser.add_argument(\"--dataset\", type=str, default=\"cifar10\", choices=[\"cifar10\", \"imagenet2012\", \"stl10\"])\n",
    "\n",
    "        (args, _) = parser.parse_known_args()\n",
    "\n",
    "        # Data\n",
    "        parser.add_argument(\"--data_dir\", type=str, default=\".\")\n",
    "        parser.add_argument(\"--num_workers\", default=8, type=int)\n",
    "\n",
    "        # optim\n",
    "        parser.add_argument(\"--batch_size\", type=int, default=256)\n",
    "        parser.add_argument(\"--learning_rate\", type=float, default=1e-3)\n",
    "        parser.add_argument(\"--weight_decay\", type=float, default=1.5e-6)\n",
    "        parser.add_argument(\"--warmup_epochs\", type=float, default=10)\n",
    "\n",
    "        # Model\n",
    "        parser.add_argument(\"--meta_dir\", default=\".\", type=str, help=\"path to meta.bin for imagenet\")\n",
    "\n",
    "        return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "656fa34b-0a92-4a2c-ac0f-14183a837187",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Sequence, Union\n",
    "\n",
    "from pytorch_lightning import Callback, LightningModule, Trainer\n",
    "from torch import Tensor\n",
    "from torch.nn import Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f5c6c20-2338-4eb6-bbe2-ad1e0d15c0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BYOLMAWeightUpdate(Callback):\n",
    "    \"\"\"Weight update rule from BYOL.\n",
    "    Your model should have:\n",
    "        - ``self.student_network``\n",
    "        - ``self.teacher_network``\n",
    "    Updates the target_network params using an exponential moving average update rule weighted by tau.\n",
    "    BYOL claims this keeps the online_network from collapsing.\n",
    "    .. note:: Automatically increases tau from ``initial_tau`` to 1.0 with every training step\n",
    "    Example::\n",
    "        # model must have 2 attributes\n",
    "        model = Model()\n",
    "        model.student_network = ...\n",
    "        model.teacher_network = ...\n",
    "        trainer = Trainer(callbacks=[BYOLMAWeightUpdate()])\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, initial_tau: float = 0.996):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            initial_tau: starting tau. Auto-updates with every training step\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.initial_tau = initial_tau\n",
    "        self.current_tau = initial_tau\n",
    "\n",
    "    def on_train_batch_end(\n",
    "        self,\n",
    "        trainer: Trainer,\n",
    "        pl_module: LightningModule,\n",
    "        outputs: Sequence,\n",
    "        batch: Sequence,\n",
    "        batch_idx: int,\n",
    "        dataloader_idx: int,\n",
    "    ) -> None:\n",
    "        # get networks\n",
    "        student_net = pl_module.student_network\n",
    "        teacher_net = pl_module.teacher_network\n",
    "\n",
    "        # update weights\n",
    "        self.update_weights(student_net, teacher_net)\n",
    "\n",
    "        # update tau after\n",
    "        self.current_tau = self.update_tau(pl_module, trainer)\n",
    "\n",
    "    def update_tau(self, pl_module: LightningModule, trainer: Trainer) -> float:\n",
    "        max_steps = len(trainer.train_dataloader) * trainer.max_epochs\n",
    "        tau = 1 - (1 - self.initial_tau) * (math.cos(math.pi * pl_module.global_step / max_steps) + 1) / 2\n",
    "        return tau\n",
    "\n",
    "    def update_weights(self, online_net: Union[Module, Tensor], target_net: Union[Module, Tensor]) -> None:\n",
    "        # apply MA weight update\n",
    "        for (name, online_p), (_, target_p) in zip(\n",
    "            online_net.named_parameters(),\n",
    "            target_net.named_parameters(),\n",
    "        ):\n",
    "            target_p.data = self.current_tau * target_p.data + (1 - self.current_tau) * online_p.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf68fed-d8b4-4b5a-a9ec-63cc8d7be385",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
